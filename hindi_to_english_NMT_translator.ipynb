{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hindi to english NMT translator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzMGWiWuVzDP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSTdvfUE2be-"
      },
      "source": [
        "#importing modules required in the model\n",
        "import csv\n",
        "#for reading csv file (Note csv is in-built module and not an external library which needs to be installed)\n",
        "\n",
        "from spacy.lang.en import English\n",
        "#for preprocessing english language\n",
        "\n",
        "from io import open     #for file management\n",
        "import sys, unicodedata, string, re, random, torch, math    #other important modules\n",
        "\n",
        "#including necessary torch functionalities\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#including matplotlib for plotting graph\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-V5jPQ0yF9C"
      },
      "source": [
        "def read_csv(path):\n",
        "    with open(path, 'r') as file:         #open csv file at location path\n",
        "        my_reader = csv.reader(file, delimiter=',')\n",
        "        cnt = 0\n",
        "        for row in my_reader:\n",
        "            if cnt == 0:    cnt += 1        #skips reading headers from csv file\n",
        "            else:\n",
        "                if str(path) == train_path:     #if train_path is argument\n",
        "                    array.append([row[1], row[2]])       #row[0] is index, row[1] is hindi, row[2] is english\n",
        "                    cnt += 1\n",
        "                else:                           #if test_path is argument\n",
        "                    sentences.append(row[2])       #row[0] is index, row[1] is index, row[2] is hindi\n",
        "                    cnt += 1\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqDy25DhvoxR"
      },
      "source": [
        "#array to store hindi-english sentence pairs\n",
        "array = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w76FRL13sBad"
      },
      "source": [
        "#importing train data from csv file\n",
        "train_path = 'drive/MyDrive/train.csv'\n",
        "cnt = read_csv(train_path)\n",
        "print(f'{cnt-1} sentences read')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMX1t1eXTUKQ"
      },
      "source": [
        "#below line is for specifying number of train pairs to be sampled randomly\n",
        "#array = random.sample(array, 45000)     #sample 45000 data pairs randomly\n",
        "#since the above line is commented, all data pairs are read\n",
        "data_hindi = []     #array to store hindi sentences\n",
        "data_english = []       #array to store english sentences\n",
        "for i in array:     #splits hindi and english sentences\n",
        "    data_hindi.append(i[0])\n",
        "    data_english.append(i[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGnGjTQwNZP-"
      },
      "source": [
        "print(len(data_hindi), len(data_english))\n",
        "print(data_hindi[:10])\n",
        "print(data_english[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pu7HRI52i0m"
      },
      "source": [
        "#installing and cloning required items for preprocessing\n",
        "get_ipython().system('pip install Morfessor')     #Morfessor models Indian languages\n",
        "\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "INDIC_NLP_LIB_HOME = r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES = \"/content/indic_nlp_resources\"\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab3QaWUA2iyp"
      },
      "source": [
        "#importing relevant items from indicnlp\n",
        "from indicnlp import loader\n",
        "from indicnlp import common\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "from indicnlp.normalize.indic_normalize import BaseNormalizer\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5c7zBiBj2z1"
      },
      "source": [
        "#enabling cuda for this model\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLgAAqRf2iur"
      },
      "source": [
        "#tokenizing English using spacy module of python\n",
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAdI9JC52itE"
      },
      "source": [
        "#defining variables\n",
        "SOS, EOS, UKN = 0, 1, 2     #start of sentence, end of sentence, unknown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWh-62Xo2iqU"
      },
      "source": [
        "#tokenize a sentence\n",
        "#using spacy for english and INDICNLP for hindi\n",
        "def tokenize_sentence(lang, s):\n",
        "    return list(tokenizer(s)) if lang.name == 'English' else list(indic_tokenize.trivial_tokenize(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI3BDGXY2ipp"
      },
      "source": [
        "#create indices\n",
        "#returns index of word in the list if word is found\n",
        "#returns 2 (the variable index for UKN) if word is not found\n",
        "def sentence_indices(lang, s):\n",
        "    return [lang.word2index.get(str(i),2) for i in list(tokenizer(s))] if lang.name == 'English' else [lang.word2index.get(i, 2) for i in list(indic_tokenize.trivial_tokenize(s))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ah6wVWV2imd"
      },
      "source": [
        "#creating variables\n",
        "#returns array of indices\n",
        "def sentence_variables(lang, s):\n",
        "    index = sentence_indices(lang, s)\n",
        "    index.append(EOS)\n",
        "    return Variable(torch.LongTensor(index).view(-1, 1)).cuda() if use_cuda else Variable(torch.LongTensor(index).view(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-WqYEx2ilT"
      },
      "source": [
        "#creating hindi-english data pairs\n",
        "#returns hindi-english pairs\n",
        "def pair_variables(hindi_language, english_language, pair):\n",
        "    return (sentence_variables(hindi_language, pair[0]), sentence_variables(english_language, pair[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2sFOs4-2iiH"
      },
      "source": [
        "#creating class Create_Language\n",
        "class Create_Language:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.vocab = 3      #dictionary size (initially 3: SOS, EOS, UKN)\n",
        "        self.word2index = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UKN\"}   #start of sentence, end of sentence, unknown\n",
        "\n",
        "    def add_word(self, word):      #adds new word to dictionary\n",
        "        if word not in self.word2index:\n",
        "            self.vocab += 1\n",
        "            self.word2index[word] = self.vocab-1\n",
        "            self.index2word[self.vocab-1] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSRkOqkL2igv"
      },
      "source": [
        "#preparing dictionary\n",
        "#returns training data pairs\n",
        "def dictionary(data_hindi, data_english):\n",
        "    hindi_language, english_language = Create_Language('Hindi'), Create_Language('English')     #creates language\n",
        "    hindi_sentences, english_sentences = list(data_hindi), list(data_english)     #data from csv file splitted into hindi and english\n",
        "    train_data_pairs = list(zip(hindi_sentences, english_sentences))        #hindi-english corresponding sentence pairing\n",
        "\n",
        "    for hindi_sentence, english_sentence in train_data_pairs:       #tokenize sentences and adding words to vocabulary/dictionary\n",
        "        for token in tokenize_sentence(hindi_language, hindi_sentence):      hindi_language.add_word(token)\n",
        "        for token in tokenize_sentence(english_language, english_sentence):      english_language.add_word(str(token))\n",
        "  \n",
        "    print(f'English has {english_language.vocab} words\\nHindi has {hindi_language.vocab} words')\n",
        "    return hindi_language, english_language, train_data_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNDTcE2M2idT"
      },
      "source": [
        "hindi_language, english_language, train_data_pairs = dictionary(data_hindi, data_english)       #displaying number of words read"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOCVDevR2ibn"
      },
      "source": [
        "#Encoder Module\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, layers = 1):    #number of layers = 1\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        for i in range(self.layers):     output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
        "        return result.cuda() if use_cuda else result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7YhiwwZ3I9T"
      },
      "source": [
        "#Decoder Module\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, layers = 1):       #number of layers = 1\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        for i in range(self.layers):\n",
        "            output = F.relu(output)\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
        "        return result.cuda() if use_cuda else result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHD4f5B03I6e"
      },
      "source": [
        "#initializing encoder and decoder\n",
        "hidden_size = 1024\n",
        "encoder = Encoder(hindi_language.vocab, hidden_size)\n",
        "decoder = Decoder(hidden_size, english_language.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azeufV-w3I3I"
      },
      "source": [
        "#using cuda for encoder and decoder\n",
        "if use_cuda:      #torch.cuda is available for use\n",
        "    encoder = encoder.cuda()\n",
        "    decoder = decoder.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2w1fBS33eYi"
      },
      "source": [
        "#parameters for train data function\n",
        "teacher_forcing_ratio = 0.6\n",
        "MAX_LENGTH = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3sUNoGh3I16"
      },
      "source": [
        "#training data function\n",
        "def train(input_variable, output_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_variable.size()[0]\n",
        "    target_length = output_variable.size()[0]\n",
        "\n",
        "    encoder_outputs = Variable(torch.zeros(input_length, encoder.hidden_size)).cuda() if use_cuda else Variable(torch.zeros(input_length, encoder.hidden_size))\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    #training encoder\n",
        "    for en_in in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_variable[en_in], encoder_hidden)\n",
        "        encoder_outputs[en_in] = encoder_output[0][0]\n",
        "\n",
        "    decoder_input = Variable(torch.LongTensor([[SOS]])).cuda() if use_cuda else Variable(torch.LongTensor([[SOS]]))\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    #training decoder based on teacher_forcing_ratio\n",
        "    if teacher_forcing:         #target output serves as next input\n",
        "        for de_in in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, output_variable[de_in])\n",
        "            decoder_input = output_variable[de_in]\n",
        "    else:        #prediction word serves as next input\n",
        "        for de_in in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            d1, d2 = decoder_output.data.topk(1)\n",
        "            decoder_input = Variable(torch.LongTensor([[d2[0][0]]])).cuda() if use_cuda else Variable(torch.LongTensor([[d2[0][0]]]))\n",
        "            loss += criterion(decoder_output, output_variable[de_in])\n",
        "            if d2[0][0] == EOS:   break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBYAvFFE3IyP"
      },
      "source": [
        "#model parameteres and some declarations\n",
        "learning_rate = 0.006\n",
        "#losses array is to record loss values per batch_size iterations so that they can be plotted later\n",
        "losses = []\n",
        "total_loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzW33aXM-PTm"
      },
      "source": [
        "#number of iterations for train phase\n",
        "iterations = 12800\n",
        "#number of epochs for train phase\n",
        "epochs = 10\n",
        "batch_size = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5augyoj3IwU"
      },
      "source": [
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr = learning_rate)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr = learning_rate)\n",
        "train_data_pairs = random.sample(train_data_pairs, iterations)\n",
        "training_pairs = [pair_variables(hindi_language, english_language, train_data_pairs[iter]) for iter in range(iterations)]      #randomly selects 12800 data pairs\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VMLtQctZm89"
      },
      "source": [
        "#function to calculate loss for training phase of the model\n",
        "def calculate_loss(iter, batch_size):\n",
        "    training_pair = training_pairs[iter]\n",
        "    input_variable, output_variable = training_pair[0], training_pair[1]\n",
        "    loss = train(input_variable, output_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1JOBD_O3Yif"
      },
      "source": [
        "#training phase for the model\n",
        "#note that this model trains on 1 data per iteration for 12800 iterations using SGD for 10 epochs\n",
        "for epoch in range(epochs):     #number of epochs\n",
        "    if epoch == 4:      #early stopping since loss increases afterwards\n",
        "        break\n",
        "    print(f'Epoch {epoch+1}:')\n",
        "    for iter in range(iterations):      #iterations = 12800 per epoch\n",
        "        total_loss += calculate_loss(iter, batch_size)\n",
        "        if (iter+1) % batch_size == 0:      #calculates average loss per batch_size = 400 iterations\n",
        "            avg_loss = total_loss / batch_size\n",
        "            losses.append(avg_loss)\n",
        "            total_loss = 0\n",
        "            print(f'    Iterations ({iter+1}/{iterations}) have an average loss of {avg_loss}')\n",
        "    plt.plot(losses)        #plots points on graph and connects them sequentially for each epoch\n",
        "    losses = []         #losses array again initialized to [] for next epoch\n",
        "    #comment above line if you want cotinuous plot of loss function and not loss per epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB3OYypy3YZ3"
      },
      "source": [
        "#function for testing data\n",
        "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
        "    input_variable = sentence_variables(hindi_language, sentence)\n",
        "    input_length = input_variable.size()[0]\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs = Variable(torch.zeros(input_length, encoder.hidden_size)).cuda() if use_cuda else Variable(torch.zeros(input_length, encoder.hidden_size))\n",
        "\n",
        "    #test input enters encoder\n",
        "    for en_in in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_variable[en_in], encoder_hidden)\n",
        "        encoder_outputs[en_in] += encoder_output[0][0]\n",
        "\n",
        "    decoder_input = Variable(torch.LongTensor([[SOS]])).cuda() if use_cuda else Variable(torch.LongTensor([[SOS]]))\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoded_words = []\n",
        "\n",
        "    #test input enters decoder\n",
        "    for de_in in range(max_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        d1, d2 = decoder_output.data.topk(1)\n",
        "        if d2[0][0].item() != EOS:\n",
        "            decoded_words.append(english_language.index2word[d2[0][0].item()])\n",
        "        else:\n",
        "            break\n",
        "        decoder_input = Variable(torch.LongTensor([[d2[0][0].item()]])).cuda() if use_cuda else Variable(torch.LongTensor([[d2[0][0].item()]]))\n",
        "\n",
        "    return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOlt_H0z5koL"
      },
      "source": [
        "#computing output using evaluate and store the result in translation\n",
        "def compute_translation(sentences):\n",
        "    translation = \"\"     #stores translated sentences in output with each sentence separated by \\n character\n",
        "    for sentence in sentences:\n",
        "        translated_sentence = ' '.join(evaluate(encoder, decoder, sentence))\n",
        "        translated_sentence += \"\\n\"\n",
        "        translation += translated_sentence\n",
        "    return translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ-6Zgsc3YKv"
      },
      "source": [
        "#writes output to answer.txt\n",
        "def write_output(output_path, translation):\n",
        "    f = open(output_path,\"w\")\n",
        "    f.write(translation)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZNTbNgAoMtp"
      },
      "source": [
        "#test phase of the model\n",
        "test_path = 'drive/MyDrive/testhindistatements.csv'\n",
        "\n",
        "sentences = []\n",
        "cnt = read_csv(test_path)\n",
        "print(f'{cnt-1} sentences read')\n",
        "\n",
        "translation = compute_translation(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quv_1-Cjz-ZS"
      },
      "source": [
        "print(sentences[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prF180Ld5kgD"
      },
      "source": [
        "output_path = 'answer.txt'\n",
        "write_output(output_path, translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JGH2Es1aPzV"
      },
      "source": [
        "#saving encoder and decoder model used for training\n",
        "model = 'encoder.pt'\n",
        "path = F\"/content/drive/MyDrive/{model}\" \n",
        "torch.save(encoder.state_dict(), path)\n",
        "model = 'decoder.pt'\n",
        "path = F\"/content/drive/MyDrive/{model}\" \n",
        "torch.save(decoder.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}